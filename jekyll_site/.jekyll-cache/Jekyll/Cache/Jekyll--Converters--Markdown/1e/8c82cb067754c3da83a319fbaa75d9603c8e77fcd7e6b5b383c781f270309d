I"b6<div class="publications">


  <h2 class="bibliography">2021</h2>
<ol class="bibliography"><li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2010.05380">
      <div class="card hoverable">
        
          


<img src="/assets/resized/WNG_RL-480x480.png" srcset="    /assets/resized/WNG_RL-480x480.png 480w,/assets/thumbnails/WNG_RL.png 779w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Moskovitz2021" class="col-sm-8">
  
    

    
      <div class="title">Efficient wasserstein natural gradients for reinforcement learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Moskovitz, Ted*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Arbel, Michael*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huszar, Ferenc,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient Wasserstein natural gradient (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including a divergence penalty in the objective to establish a trust region. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2010.05380" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/tedmoskovitz/WNPG" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_WNPG.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
      
      <a href="/assets/pdf/talk_WNPG.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Moskovitz2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient wasserstein natural gradients for reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moskovitz, Ted* and Arbel, Michael* and Huszar, Ferenc and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2010.05380}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/tedmoskovitz/WNPG}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_WNPG.pdf}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{talk_WNPG.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{WNG_RL.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2101.07528">
      <div class="card hoverable">
        
          


<img src="/assets/resized/Patches-1400x1400.png" srcset="    /assets/resized/Patches-480x480.png 480w,    /assets/resized/Patches-800x800.png 800w,    /assets/resized/Patches-1400x1400.png 1400w,/assets/thumbnails/Patches.png 6040w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Thiry2021" class="col-sm-8">
  
    

    
      <div class="title">The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Thiry, Louis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Belilovsky, Eugene,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Oyallon, Edouard
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A recent line of work showed that various forms of convolutional kernel methods can be competitive with standard supervised deep convolutional networks on datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while being more amenable to theoretical analysis. In this work, we highlight the importance of a data-dependent feature extraction step that is key to the obtain good performance in convolutional kernel methods. This step typically corresponds to a whitened dictionary of patches, and gives rise to a data-driven convolutional kernel methods. We extensively study its effect, demonstrating it is the key ingredient for high performance of these methods. Specifically, we show that one of the simplest instances of such kernel methods, based on a single layer of image patches followed by a linear classifier is already obtaining classification accuracies on CIFAR-10 in the same range as previous more sophisticated convolutional kernel methods. We scale this method to the challenging ImageNet dataset, showing such a simple approach can exceed all existing non-learned representation methods. This is a new baseline for object recognition without representation learning methods, that initiates the investigation of convolutional kernel models on ImageNet. We conduct experiments to analyze the dictionary that we used, our ablations showing they exhibit low-dimensional properties.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2101.07528" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/louity/patches" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_patches.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
      
      <a href="/assets/pdf/talk_patches.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Thiry2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thiry, Louis and Arbel, Michael and Belilovsky, Eugene and Oyallon, Edouard}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2101.07528}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/louity/patches}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_patches.pdf}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{talk_patches.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{Patches.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2111.02994">
      <div class="card hoverable">
        
          


<img src="/assets/resized/GEBM-1400x1400.png" srcset="    /assets/resized/GEBM-480x480.png 480w,    /assets/resized/GEBM-800x800.png 800w,    /assets/resized/GEBM-1400x1400.png 1400w,/assets/thumbnails/GEBM.png 3020w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel2021" class="col-sm-8">
  
    

    
      <div class="title">Generalized energy based models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhou, Liang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Much of the recent success of deep reinforcement learning has been driven by regularized policy optimization (RPO) algorithms, with strong performance across multiple domains. In this family of methods, agents are trained to maximize cumulative reward while penalizing deviation in behavior from some reference, or default policy. In addition to empirical success, there is a strong theoretical foundation for understanding RPO methods applied to single tasks, with connections to natural gradient, trust region, and variational approaches. However, there is limited formal understanding of desirable properties for default policies in the multitask setting, an increasingly important domain as the field shifts towards training more generally capable agents. Here, we take a first step towards filling this gap by formally linking the quality of the default policy to its effect on optimization. Using these results, we then derive a principled RPO algorithm for multitask learning with strong performance guarantees.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2111.02994" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/GeneralizedEBM" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_GEBM.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
      
      <a href="/assets/pdf/talk_GEBM.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel2021</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalized energy based models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael and Zhou, Liang and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2111.02994}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/GeneralizedEBM}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_GEBM.pdf}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{talk_GEBM.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{GEBM.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2102.07501">
      <div class="card hoverable">
        
          


<img src="" srcset="/assets/thumbnails/AFT.png 443w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel2021a" class="col-sm-8">
  
    

    
      <div class="title">Annealed Flow Transport Monte Carlo</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Arbel, Michael*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Matthews, Alexander GDG*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Doucet, Arnaud
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of Machine Learning Research</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC) extensions are state-of-the-art methods for estimating normalizing constants of probability distributions. We propose here a novel Monte Carlo algorithm, Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them with normalizing flows (NFs) for improved performance. This method transports a set of particles using not only importance sampling (IS), Markov chain Monte Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are learned sequentially to push particles towards the successive annealed targets. We provide limit theorems for the resulting Monte Carlo estimates of the normalizing constant and expectations with respect to the target distribution. Additionally, we show that a continuous-time scaling limit of the population version of AFT is given by a Feynman–Kac measure which simplifies to the law of a controlled diffusion for expressive NFs. We demonstrate experimentally the benefits and limitations of our methodology on a variety of applications.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2102.07501" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/deepmind/annealed_flow_transport" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_AFT.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
      
      <a href="/assets/pdf/talk_AFT.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel2021a</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICML}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Annealed Flow Transport Monte Carlo}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael* and Matthews, Alexander GDG* and Doucet, Arnaud}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2102.07501}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/deepmind/annealed_flow_transport}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_AFT.pdf}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{talk_AFT.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{AFT.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2102.03765">
      <div class="card hoverable">
        
          


<img src="/assets/resized/TOP-1400x1400.png" srcset="    /assets/resized/TOP-480x480.png 480w,    /assets/resized/TOP-800x800.png 800w,    /assets/resized/TOP-1400x1400.png 1400w,/assets/thumbnails/TOP.png 2758w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Moskovitz2021a" class="col-sm-8">
  
    

    
      <div class="title">Tactical Optimism and Pessimism for Deep Reinforcement Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Moskovitz, Ted,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Parker-Holder, Jack,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pacchiano, Aldo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jordan, Michael I
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. One of the primary drivers of this improved performance is the use of pessimistic value updates to address function approximation errors, which previously led to disappointing performance. However, a direct consequence of pessimism is reduced exploration, running counter to theoretical support for the efficacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the most effective degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP) estimation, which switches between optimistic and pessimistic value learning online. This is achieved by formulating the selection as a multi-arm bandit problem. We show in a series of continuous control tasks that TOP outperforms existing methods which rely on a fixed degree of optimism, setting a new state of the art in challenging pixel-based environments. Since our changes are simple to implement, we believe these insights can easily be incorporated into a multitude of off-policy algorithms.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2102.03765" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/tedmoskovitz/TOP" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Moskovitz2021a</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tactical Optimism and Pessimism for Deep Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moskovitz, Ted and Parker-Holder, Jack and Pacchiano, Aldo and Arbel, Michael and Jordan, Michael I}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2102.03765}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/tedmoskovitz/TOP}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{TOP.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2106.08929">
      <div class="card hoverable">
        
          


<img src="/assets/resized/KALE-480x480.png" srcset="    /assets/resized/KALE-480x480.png 480w,/assets/thumbnails/KALE.png 556w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Glaser2021" class="col-sm-8">
  
    

    
      <div class="title">KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Glaser, Pierre,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergence between a moving source and a fixed target distribution. This approximation, termed the KALE (KL approximate lower-bound estimator), solves a regularized version of the Fenchel dual problem defining the KL over a restricted class of functions. When using a Reproducing Kernel Hilbert Space (RKHS) to define the function class, we show that the KALE continuously interpolates between the KL and the Maximum Mean Discrepancy (MMD). Like the MMD and other Integral Probability Metrics, the KALE remains well defined for mutually singular distributions. Nonetheless, the KALE inherits from the limiting KL a greater sensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the KALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE’s properties.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2106.08929" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Glaser2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Glaser, Pierre and Arbel, Michael and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2106.08929}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{KALE.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2111.02994">
      <div class="card hoverable">
        
          


<img src="/assets/resized/Default_Policy_RL-800x800.png" srcset="    /assets/resized/Default_Policy_RL-480x480.png 480w,    /assets/resized/Default_Policy_RL-800x800.png 800w,/assets/thumbnails/Default_Policy_RL.png 871w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Moskovitz2021b" class="col-sm-8">
  
    

    
      <div class="title">Towards an Understanding of Default Policies in Multitask Policy Optimization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Moskovitz, Ted,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Parker-Holder, Jack,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pacchiano, Aldo
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2111.02994</em>
      
      
      
        2021
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Much of the recent success of deep reinforcement learning has been driven by regularized policy optimization (RPO) algorithms, with strong performance across multiple domains. In this family of methods, agents are trained to maximize cumulative reward while penalizing deviation in behavior from some reference, or default policy. In addition to empirical success, there is a strong theoretical foundation for understanding RPO methods applied to single tasks, with connections to natural gradient, trust region, and variational approaches. However, there is limited formal understanding of desirable properties for default policies in the multitask setting, an increasingly important domain as the field shifts towards training more generally capable agents. Here, we take a first step towards filling this gap by formally linking the quality of the default policy to its effect on optimization. Using these results, we then derive a principled RPO algorithm for multitask learning with strong performance guarantees.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2111.02994" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Moskovitz2021b</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards an Understanding of Default Policies in Multitask Policy Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moskovitz, Ted and Arbel, Michael and Parker-Holder, Jack and Pacchiano, Aldo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2111.02994}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2111.02994}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{Default_Policy_RL.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1910.09652">
      <div class="card hoverable">
        
          


<img src="/assets/resized/KWNG-800x800.png" srcset="    /assets/resized/KWNG-480x480.png 480w,    /assets/resized/KWNG-800x800.png 800w,/assets/thumbnails/KWNG.png 931w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel:2020" class="col-sm-8">
  
    

    
      <div class="title">Kernelized Wasserstein Natural Gradient</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gretton, Arthur,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Wuchen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Montufar, Guido
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations </em>
      
      
      
        2020
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Barycentric averaging is a principled way of summarizing populations of measures. Existing algorithms for estimating barycenters typically parametrize them as weighted sums of Diracs and optimize their weights and/or locations. However, these approaches do not scale to high-dimensional settings due to the curse of dimensionality. In this paper, we propose a scalable and general algorithm for estimating barycenters of measures in high dimensions. The key idea is to turn the optimization over measures into an optimization over generative models, introducing inductive biases that allow the method to scale while still accurately estimating barycenters. We prove local convergence under mild assumptions on the discrepancy showing that the approach is well-posed. We demonstrate that our method is fast, achieves good performance on low-dimensional problems, and scales to high-dimensional settings. In particular, our approach is the first to be used to estimate barycenters in thousands of dimensions.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1910.09652" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/KWNG" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
    
      
      <a href="/assets/pdf/slides_KWNG.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel:2020</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael and Gretton, Arthur and Li, Wuchen and Montufar, Guido}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kernelized Wasserstein Natural Gradient}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1910.09652}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/KWNG}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{slides_KWNG.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{KWNG.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2004.00663">
      <div class="card hoverable">
        
          


<img src="/assets/resized/OT_sync-480x480.png" srcset="    /assets/resized/OT_sync-480x480.png 480w,/assets/thumbnails/OT_sync.png 563w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Tolga:2020" class="col-sm-8">
  
    

    
      <div class="title">Synchronizing Probability Measures on Rotations via Optimal Transport</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Birdal, Tolga,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Simsekli, Umut,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Guibas, Leonidas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>
      
      
      
        2020
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce a new paradigm, measure synchronization, for synchronizing graphs with measure-valued edges. We formulate this problem as maximization of the cycle-consistency in the space of probability measures over relative rotations. In particular, we aim at estimating marginal distributions of absolute orientations by synchronizing the conditional ones, which are defined on the Riemannian manifold of quaternions. Such graph optimization on distributions-on-manifolds enables a natural treatment of multimodal hypotheses, ambiguities and uncertainties arising in many computer vision applications such as SLAM, SfM, and object pose estimation. We first formally define the problem as a generalization of the classical rotation graph synchronization, where in our case the vertices denote probability measures over rotations. We then measure the quality of the synchronization by using Sinkhorn divergences, which reduces to other popular metrics such as Wasserstein distance or the maximum mean discrepancy as limit cases. We propose a nonparametric Riemannian particle optimization approach to solve the problem. Even though the problem is non-convex, by drawing a connection to the recently proposed sparse optimization methods, we show that the proposed algorithm converges to the global optimum in a special case of the problem under certain conditions. Our qualitative and quantitative experiments show the validity of our approach and we bring in new perspectives to the study of synchronization.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2004.00663" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/OT-sync" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
    
    
      <a href="https://synchinvision.github.io/probsync/" class="btn btn-sm z-depth-0" role="button">Website</a>
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Tolga:2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Birdal, Tolga and Arbel, Michael and Simsekli, Umut and Guibas, Leonidas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Synchronizing Probability Measures on Rotations via Optimal Transport}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2004.00663}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/OT-sync}</span><span class="p">,</span>
  <span class="na">website</span> <span class="p">=</span> <span class="s">{https://synchinvision.github.io/probsync/}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{OT_sync.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2006.09797">
      <div class="card hoverable">
        
          


<img src="/assets/resized/SVGD-480x480.png" srcset="    /assets/resized/SVGD-480x480.png 480w,/assets/thumbnails/SVGD.png 556w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Korba2020" class="col-sm-8">
  
    

    
      <div class="title">A non-asymptotic analysis for Stein variational gradient descent</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Korba, Anna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Salim, Adil,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Luise, Giulia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
      
        2020
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the Stein Variational Gradient Descent (SVGD) algorithm, which optimises a set of particles to approximate a target probability distribution π∝e−V on ℝd. In the population limit, SVGD performs gradient descent in the space of probability distributions on the KL divergence with respect to π, where the gradient is smoothed through a kernel integral operator. In this paper, we provide a novel finite time analysis for the SVGD algorithm. We provide a descent lemma establishing that the algorithm decreases the objective at each iteration, and rates of convergence for the average Stein Fisher divergence (also referred to as Kernel Stein Discrepancy). We also provide a convergence result of the finite particle system corresponding to the practical implementation of SVGD to its population version.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2006.09797" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
      
      <a href="/assets/pdf/poster_SVGD.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Korba2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A non-asymptotic analysis for Stein variational gradient descent}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Korba, Anna and Salim, Adil and Arbel, Michael and Luise, Giulia and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2006.09797}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_SVGD.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{SVGD.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/2007.07105">
      <div class="card hoverable">
        
          


<img src="/assets/resized/Barycenter-800x800.png" srcset="    /assets/resized/Barycenter-480x480.png 480w,    /assets/resized/Barycenter-800x800.png 800w,/assets/thumbnails/Barycenter.png 931w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Cohen2020" class="col-sm-8">
  
    

    
      <div class="title">Estimating barycenters of measures in high dimensions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Cohen, Samuel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Deisenroth, Marc Peter
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2007.07105</em>
      
      
      
        2020
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Barycentric averaging is a principled way of summarizing populations of measures. Existing algorithms for estimating barycenters typically parametrize them as weighted sums of Diracs and optimize their weights and/or locations. However, these approaches do not scale to high-dimensional settings due to the curse of dimensionality. In this paper, we propose a scalable and general algorithm for estimating barycenters of measures in high dimensions. The key idea is to turn the optimization over measures into an optimization over generative models, introducing inductive biases that allow the method to scale while still accurately estimating barycenters. We prove local convergence under mild assumptions on the discrepancy showing that the approach is well-posed. We demonstrate that our method is fast, achieves good performance on low-dimensional problems, and scales to high-dimensional settings. In particular, our approach is the first to be used to estimate barycenters in thousands of dimensions.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2007.07105" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Cohen2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Estimating barycenters of measures in high dimensions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cohen, Samuel and Arbel, Michael and Deisenroth, Marc Peter}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2007.07105}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2007.07105}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{Barycenter.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1906.04370">
      <div class="card hoverable">
        
          


<img src="/assets/resized/MMD-flow-480x480.png" srcset="    /assets/resized/MMD-flow-480x480.png 480w,/assets/thumbnails/MMD-flow.png 594w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel:2019" class="col-sm-8">
  
    

    
      <div class="title">Maximum Mean Discrepancy Gradient Flow</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Korba, Anna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Salim, Adil,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
      
        2019
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We construct a Wasserstein gradient flow of the maximum mean discrepancy (MMD) and study its convergence properties.
The MMD is an integral probability metric defined for a reproducing kernel Hilbert space (RKHS), and serves as a metric on probability measures for a sufficiently rich RKHS. We obtain conditions for convergence of the gradient flow towards a global optimum, that can be related to particle transport when optimizing neural networks.
We also propose a way to regularize this MMD flow, based on an injection of noise in the gradient. This algorithmic fix comes with theoretical and empirical evidence. The practical implementation of the flow is straightforward, since both the MMD and its gradient have simple closed-form expressions, which can be easily estimated with samples.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1906.04370" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/MMD-gradient-flow" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_MMD_flow.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel:2019</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael and Korba, Anna and Salim, Adil and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Maximum Mean Discrepancy Gradient Flow}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1906.04370}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/MMD-gradient-flow}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_MMD_flow.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{MMD-flow.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1711.05363">
      <div class="card hoverable">
        
          


<img src="" srcset="/assets/thumbnails/KCEF.png 404w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel:2018" class="col-sm-8">
  
    

    
      <div class="title">Kernel Conditional Exponential Family</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 21th International Conference on Artificial Intelligence and Statistics</em>
      
      
      
        2018
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A nonparametric family of conditional distributions is introduced, which generalizes conditional exponential families using functional parameters in a suitable RKHS. An algorithm is provided for learning the generalized natural parameter, and consistency of the estimator is established in the well specified case. In experiments, the new method generally outperforms a competing approach with consistency guarantees, and is competitive with a deep conditional density model on datasets that exhibit abrupt transitions and heteroscedasticity.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1711.05363" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/KCEF" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_KCEF.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel:2018</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{AISTATS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kernel Conditional Exponential Family}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 21th International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1711.05363}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/KCEF}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_KCEF.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{KCEF.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1705.08360">
      <div class="card hoverable">
        
          


<img src="/assets/resized/KExpFam-800x780.png" srcset="    /assets/resized/KExpFam-480x468.png 480w,    /assets/resized/KExpFam-800x780.png 800w,/assets/thumbnails/KExpFam.png 859w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Sutherland:2018" class="col-sm-8">
  
    

    
      <div class="title">Efficient and principled score estimation with Nystrom kernel exponential families</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sutherland, Danica J.*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Strathmann, Heiko*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the 21th International Conference on Artificial Intelligence and Statistics</em>
      
      
      
        2018
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite-dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nyström-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1705.08360" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/karlnapf/nystrom-kexpfam" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_ExpFam.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Sutherland:2018</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{AISTATS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sutherland, Danica J.* and Strathmann, Heiko* and Arbel, Michael and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient and principled score estimation with Nystrom kernel exponential families}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 21th International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1705.08360}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/karlnapf/nystrom-kexpfam}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_ExpFam.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{KExpFam.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1801.01401">
      <div class="card hoverable">
        
          


<img src="/assets/resized/MMD-GAN-800x800.png" srcset="    /assets/resized/MMD-GAN-480x480.png 480w,    /assets/resized/MMD-GAN-800x800.png 800w,/assets/thumbnails/MMD-GAN.png 836w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Binkowski:2018" class="col-sm-8">
  
    

    
      <div class="title">Demystifying MMD GANs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Bińkowski, Mikołaj*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sutherland, Danica J.*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Arbel, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations</em>
      
      
      
        2018
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1801.01401" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/mbinkowski/MMD-GAN" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_MMD_GAN.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Binkowski:2018</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bińkowski, Mikołaj* and Sutherland, Danica J.* and Arbel, Michael and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Demystifying {MMD} {GAN}s}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1801.01401}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/mbinkowski/MMD-GAN}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_MMD_GAN.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{MMD-GAN.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li>
<li><div class="row">

  <div class="col-sm-2 abbr">
  
  
  
    
      <div class="grid-item">
    <a href="http://arxiv.org/abs/1805.11565">
      <div class="card hoverable">
        
          


<img src="/assets/resized/SMMD-800x800.png" srcset="    /assets/resized/SMMD-480x480.png 480w,    /assets/resized/SMMD-800x800.png 800w,/assets/thumbnails/SMMD.png 952w" alt="entry.arxiv" />

        
      </div>
    </a>
    </div>
    
    
  
  
  </div>

  <div id="Arbel:2018a" class="col-sm-8">
  
    

    
      <div class="title">On gradient regularizers for MMD GANs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Arbel, Michael*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sutherland, Danica J.*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bińkowski, Mikołaj,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gretton, Arthur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Advances in Neural Information Processing Systems</em>
      
      
      
        2018
      
      </div>

    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160×160 CelebA and 64×64 unconditional ImageNet.</p>
    </div>
    
    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1805.11565" class="btn btn-sm z-depth-0" role="button">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
    
    
    
      <a href="https://github.com/MichaelArbel/Scaled-MMD-GAN" class="btn btn-sm z-depth-0" role="button">Code</a>
    
    
      
      <a href="/assets/pdf/poster_SMMD_GAN.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    </div>



    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Arbel:2018a</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arbel, Michael* and Sutherland, Danica J.* and Bińkowski, Mikołaj and Gretton, Arthur}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On gradient regularizers for MMD GANs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1805.11565}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MichaelArbel/Scaled-MMD-GAN}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{poster_SMMD_GAN.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{SMMD.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
    
    
  </div>

</div>
</li></ol>


</div>
:ET